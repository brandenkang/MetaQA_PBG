{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchbiggraph.config import parse_config\n",
    "from torchbiggraph.converters.importers import TSVEdgelistReader, convert_input_data\n",
    "from torchbiggraph.train import train\n",
    "from torchbiggraph.util import SubprocessInitializer, setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/example_2'\n",
    "GRAPH_PATH = DATA_DIR + '/edges.tsv'\n",
    "TRAINING_PATH = DATA_DIR + '/training.tsv'\n",
    "TEST_PATH = DATA_DIR + '/test.tsv'\n",
    "MODEL_DIR = 'model_2'\n",
    "\n",
    "config = dict(\n",
    "    # I/O data\n",
    "    entity_path=DATA_DIR,\n",
    "    edge_paths=[\n",
    "        DATA_DIR + '/edge_path'\n",
    "    ],\n",
    "    checkpoint_path=MODEL_DIR,\n",
    "    # Graph structure\n",
    "    entities={\"all\": {\"num_partitions\": 1}},\n",
    "    relations=[\n",
    "       {\n",
    "            \"name\": \"all_edges\",\n",
    "            \"lhs\": \"all\",\n",
    "            \"rhs\": \"all\",\n",
    "            \"operator\": \"translation\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    dynamic_relations=True,\n",
    "    dimension=4,  \n",
    "    global_emb=False,\n",
    "    comparator=\"dot\",\n",
    "    num_epochs=7,\n",
    "    num_uniform_negs=1000,\n",
    "    loss_fn=\"softmax\",\n",
    "    lr=0.1,\n",
    "    regularization_coef=1e-3,\n",
    "    eval_fraction=0.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 2. TRANSFORM GRAPH TO A BIGGRAPH-FRIENDLY FORMAT\n",
    "# This step generates the following metadata files:\n",
    "\n",
    "# data/example_2/entity_count_director_0.txt\n",
    "# data/example_2/entity_count_director_0.json\n",
    "\n",
    "# and this file with data:\n",
    "# data/example_2/edges_partitioned/edges_0_0.h5\n",
    "# =================================================\n",
    "setup_logging()\n",
    "config = parse_config(config)\n",
    "subprocess_init = SubprocessInitializer()\n",
    "input_edge_paths = [Path(GRAPH_PATH)]\n",
    "\n",
    "convert_input_data(\n",
    "    config.entities,\n",
    "    config.relations,\n",
    "    config.entity_path,\n",
    "    config.edge_paths,\n",
    "    input_edge_paths,\n",
    "    TSVEdgelistReader(lhs_col=0, rel_col=1, rhs_col=2),\n",
    "    dynamic_relations=config.dynamic_relations,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15 16:14:57,151   [Trainer-0] Loading entity counts...\n",
      "2021-04-15 16:14:57,648   [Trainer-0] Creating workers...\n",
      "2021-04-15 16:14:57,854   [Trainer-0] Initializing global model...\n",
      "2021-04-15 16:14:59,129   [Trainer-0] Exiting\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# 3. TRAIN THE EMBEDDINGS\n",
    "# files generated in this step:\n",
    "#\n",
    "# checkpoint_version.txt\n",
    "# config.json\n",
    "# embeddings_all_0.v7.h5\n",
    "# model.v7.h5\n",
    "# training_stats.json\n",
    "# ===============================================\n",
    "\n",
    "train(config, subprocess_init=subprocess_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/data/example_2/entity_names_all_0.json', 'r') as f:\n",
    "    embeddings = json.load(f)\n",
    "\n",
    "with h5py.File('/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/embeddings_all_0.v7.h5', 'r') as g:\n",
    "    embeddings_all = g['embeddings'][:]\n",
    "\n",
    "embedding_final = dict(zip(embeddings, embeddings_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Group' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e52745d8338b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/model.v7.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# with h5py.File(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/model.v7.h5\", \"r\") as hf:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     operator_state_dict = {\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Group' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "with h5py.File('/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/model.v7.h5', 'r') as g:\n",
    "    print(g['model'].encode('ascii')[:])\n",
    "\n",
    "# with h5py.File(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/model.v7.h5\", \"r\") as hf:\n",
    "#     operator_state_dict = {\n",
    "#         \"real\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/real\"][...]),\n",
    "#         \"imag\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/imag\"][...]),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48531008 -0.02907323  0.409445   -0.27130792]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_final['Pal_Joey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "# import faiss\n",
    "\n",
    "# # Create FAISS index\n",
    "# index = faiss.IndexFlatL2(400)\n",
    "# with h5py.File(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/embeddings_all_0.v7.h5\", \"r\") as hf:\n",
    "#     index.add(hf[\"embeddings\"][...])\n",
    "\n",
    "# # Get trained embedding of Paris\n",
    "# with open(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/data/example_2/entity_names_all_0.json\", \"rt\") as tf:\n",
    "#     entity_names = json.load(tf)\n",
    "# target_entity_offset = entity_names.index(\"/m/05qtj\")  # Paris\n",
    "# with h5py.File(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/embeddings_all_0.v7.h5\", \"r\") as hf:\n",
    "#     target_embedding = hf[\"embeddings\"][target_entity_offset, :]\n",
    "\n",
    "# print(target_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ComplexDiagonalDynamicOperator' from 'torchbiggraph.model' (/Users/BrandenKang/anaconda3/lib/python3.7/site-packages/torchbiggraph/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-1934522b0879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchbiggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexDiagonalDynamicOperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDotComparator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load entity count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ComplexDiagonalDynamicOperator' from 'torchbiggraph.model' (/Users/BrandenKang/anaconda3/lib/python3.7/site-packages/torchbiggraph/model.py)"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import h5py\n",
    "# import torch\n",
    "# from torchbiggraph.model import ComplexDiagonalDynamicOperator, DotComparator\n",
    "\n",
    "# # Load entity count\n",
    "# with open(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/data/example_2/entity_count_all_0.txt\", \"rt\") as tf:\n",
    "#     entity_count = int(tf.read().strip())\n",
    "\n",
    "# # Load count of dynamic relations\n",
    "# with open(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/data/example_2/dynamic_rel_count.txt\", \"rt\") as tf:\n",
    "#     dynamic_rel_count = int(tf.read().strip())\n",
    "\n",
    "# # Load the operator's state dict\n",
    "# with h5py.File(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/model.v7.h5\", \"r\") as hf:\n",
    "#     operator_state_dict = {\n",
    "#         \"real\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/real\"][...]),\n",
    "#         \"imag\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/imag\"][...]),\n",
    "#     }\n",
    "# operator = ComplexDiagonalDynamicOperator(4, dynamic_rel_count)\n",
    "# operator.load_state_dict(operator_state_dict)\n",
    "# comparator = DotComparator()\n",
    "\n",
    "# # Load the offsets of the entities and the index of the relation type\n",
    "# with open(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/data/example_2/entity_names_all_0.json\", \"rt\") as tf:\n",
    "#     entity_names = json.load(tf)\n",
    "# src_entity_offset = entity_names.index(\"/m/0f8l9c\")  # France\n",
    "# with open(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/data/example_2/dynamic_rel_names.json\", \"rt\") as tf:\n",
    "#     rel_type_names = json.load(tf)\n",
    "# rel_type_index = rel_type_names.index(\"/location/country/capital\")\n",
    "\n",
    "# # Load the trained embeddings\n",
    "# with h5py.File(\"/Users/BrandenKang/Documents/GitHub/MetaQA_PBG/model_2/embeddings_all_0.v7.h5\", \"r\") as hf:\n",
    "#     src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\n",
    "#     dest_embeddings = torch.from_numpy(hf[\"embeddings\"][...])\n",
    "\n",
    "# # Calculate the scores\n",
    "# scores, _, _ = comparator(\n",
    "#     comparator.prepare(src_embedding.view(1, 1, 4)).expand(1, entity_count, 4),\n",
    "#     comparator.prepare(\n",
    "#         operator(\n",
    "#             dest_embeddings,\n",
    "#             torch.tensor([rel_type_index]).expand(entity_count),\n",
    "#         ).view(1, entity_count, 400),\n",
    "#     ),\n",
    "#     torch.empty(1, 0, 4),  # Left-hand side negatives, not needed\n",
    "#     torch.empty(1, 0, 4),  # Right-hand side negatives, not needed\n",
    "# )\n",
    "\n",
    "# # Sort the entities by their score\n",
    "# permutation = scores.flatten().argsort(descending=True)\n",
    "# top5_entities = [entity_names[index] for index in permutation[:5]]\n",
    "\n",
    "# print(top5_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze\n",
    "# !pip install gensim==4.0.0b0\n",
    "# !pip install --upgrade gensim\n",
    "# gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {list(entity2embedding.keys())[i]: list(entity2embedding.values())[i] for i in ranypege(len(list_1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONEncoder\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.json','w') as fp:\n",
    "    json.dump(a, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2embedding\n",
    "list_1 = entity2embedding.keys()\n",
    "list_1 = list(list_1)\n",
    "list_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BrandenKang/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3185: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24713, 200)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_2 = entity2embedding.values()\n",
    "## create 2D array out of list_2 dict values -- put in in a variable (embeddings)\n",
    "embeddings = np.stack(list_2, axis=0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec, KeyedVectors   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_models = KeyedVectors(4,count=len(list_1))\n",
    "gensim_models.add_vectors(list_1,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_models.save('gensim_model.model') #bin #kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add relationships\n",
    "result = gensim_models.most_similar(positive=['Drumline', ],topn=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Icon'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec, KeyedVectors   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Free_Enterprise'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_gensim = KeyedVectors.load('gensim_model.model')\n",
    "test = re_gensim.most_similar(positive=['Cobra_Woman', 'Sleep_with_Me'],topn=1)[0][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first need to have the relationship in the form of embeddings \n",
    "## King + Spouse = most related entity (how the translation works)\n",
    "## how translation works: (take a vector and another vector, and take the sum) \n",
    "## translation model is a model where you take left hand side and shift it by the relationship and you hope you get close to right hand side\n",
    "## as opposed to rotation or another kind of vector manipulation \n",
    "\n",
    "## can use gensim by saying this is the lhs, this is the relationship, take them as features using positive keyword\n",
    "## hoping we can play with that (positive, negative)\n",
    "\n",
    "## if we do not have information about relationship we cannot do translation \n",
    "## it should train entities of embeddings and also train relationship and provide them \n",
    "\n",
    "## how big is the model \n",
    "\n",
    "## goal is to experiment and see where relationship is (make repository with 4 dimensions and put to github)\n",
    "## configuration file, output files and have a look \n",
    "## left hand side, right hand side, try to position vectors so that when you add up lhs and relationship you get rhs\n",
    "## in process you teak embedding of LHS and relationship, such that you get close to RHS \n",
    "\n",
    "## in the meantime, try to integrate what you have \n",
    "## you have bot interface, you have very simple prediction model of similarity\n",
    "## try to integrate them — spielberg – output is most similar stuff \n",
    "## and then leveredge positive keyword of most similar method — in case query is multiple entities \n",
    "## i.e. spielberg, and jurassic park — use those as positive and get most similar \n",
    "## just to create pipeline — and then plug in relationship part once it's figured out \n",
    "\n",
    "## next time ideally we'll create an instance on AWS or Heroku and we can put application up there\n",
    "\n",
    "## deadlines: \n",
    "## Beginning of April, Sunday — finish 95% of development\n",
    "## I have April to do fine tuning, and have time to prepare for report and presentation \n",
    "## first of all ahve to think about what i'm going to put inside \n",
    "## I will have to have significant part talking about theoretical things I have learned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.save_word2vec_format(entity2embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = 'data/example_2'\n",
    "# GRAPH_PATH = DATA_DIR + '/edges.tsv'\n",
    "# MODEL_DIR = 'model_2'\n",
    "\n",
    "#     # ==================================================================\n",
    "#     # 0. PREPARE THE GRAPH\n",
    "#     # the result of this step is a single file 'data/example_2/graph.tsv'\n",
    "#     # ==================================================================\n",
    "#     # This the graph we will be embedding.\n",
    "#     # It has 10 types of nodes, or entities, and 9 types of edges, or relationships. \n",
    "#     test_edges = []\n",
    "#     count=0\n",
    "#     with open('kb.txt', 'r') as f: \n",
    "#         for line in f: \n",
    "#            line=line.rstrip().split(\"|\")\n",
    "#            line[0] = line[0].split(\" \")\n",
    "#            line[0] = \"_\".join(line[0])\n",
    "#         #    line[2] = line[2].split(\" \")\n",
    "#         #    line[2] = \"_\".join(line[2])\n",
    "#            test_edges.append(line)\n",
    "#            count+=1\n",
    "#            if count == 134741:\n",
    "#                break\n",
    "           \n",
    "#     os.makedirs(DATA_DIR, exist_ok=True)\n",
    "#     with open(GRAPH_PATH, 'w') as f:\n",
    "#         for edge in test_edges:\n",
    "#             f.write('\\t'.join(edge) + '\\n')\n",
    "# # # # ==================================================\n",
    "# # # # 1. DEFINE CONFIG\n",
    "# # # # this dictionary will be used in steps 2. and 3.\n",
    "# # # # ==================================================\n",
    "\n",
    "# raw_config = dict(\n",
    "#     # I/O data\n",
    "#     entity_path=DATA_DIR,\n",
    "#     edge_paths=[\n",
    "#         DATA_DIR + '/edges_partitioned',\n",
    "#     ],\n",
    "#     checkpoint_path=MODEL_DIR,\n",
    "#     # Graph structure\n",
    "#     entities={\n",
    "#         \"all\": {\"num_partitions\": 1}\n",
    "#     },\n",
    "#     relations=[\n",
    "#         {\n",
    "#             \"name\": \"directed_by\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"name\": \"written_by\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"name\": \"starred_actors\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             \"name\": \"release_year\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"name\": \"in_language\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"name\": \"has_tags\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"name\": \"has_genre\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             \"name\": \"has_imdb_votes\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             \"name\": \"all_edges\",\n",
    "#             \"lhs\": \"all\",\n",
    "#             \"rhs\": \"all\",\n",
    "#             \"operator\": \"complex_diagonal\",\n",
    "#         }\n",
    "#     ],\n",
    "\n",
    "#     dynamic_relations=False,\n",
    "#     dimension=200,  \n",
    "#     global_emb=False,\n",
    "#     comparator=\"dot\",\n",
    "#     num_epochs=7,\n",
    "#     num_uniform_negs=1000,\n",
    "#     loss_fn=\"softmax\",\n",
    "#     lr=0.1,\n",
    "#     regularization_coef=1e-3,\n",
    "#     eval_fraction=0.,\n",
    "# )\n",
    "\n",
    "## Set Up Logging \n",
    "\n",
    "\n",
    "# # =======================================================================\n",
    "# # 4. LOAD THE EMBEDDINGS\n",
    "# # The final output of the process consists of a dictionary mapping each entity to its embedding\n",
    "\n",
    "# # =======================================================================\n",
    "\n",
    "# # entities_path = DATA_DIR + '/entity_names_entities_0.json'\n",
    "\n",
    "# # entities_emb_path = MODEL_DIR + \"/embeddings_entities.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "# #     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# # with open(entities_path, 'r') as f:\n",
    "# #     entities = json.load(f)\n",
    "\n",
    "# # with h5py.File(entities_emb_path, 'r') as g:\n",
    "# #     entity_embeddings = g['embeddings'][:]\n",
    "\n",
    "# # entity2embedding = dict(zip(entities, entity_embeddings))\n",
    "# # print('entity embeddings')\n",
    "# # print(entity2embedding)\n",
    "\n",
    "# movies_path = DATA_DIR + '/entity_names_movie_0.json'\n",
    "# directors_path = DATA_DIR + '/entity_names_director_0.json'\n",
    "# writers_path = DATA_DIR + '/entity_names_writer_0.json'\n",
    "# actors_path = DATA_DIR + '/entity_names_starred_actor_0.json'\n",
    "# years_path = DATA_DIR + '/entity_names_year_0.json'\n",
    "# languages_path = DATA_DIR + '/entity_names_language_0.json'\n",
    "# tags_path = DATA_DIR + '/entity_names_tags_0.json'\n",
    "# genres_path = DATA_DIR + '/entity_names_genre_0.json'\n",
    "# votes_path = DATA_DIR + '/entity_names_votes_0.json'\n",
    "# rating_path = DATA_DIR + '/entity_names_rating_0.json'\n",
    "\n",
    "\n",
    "# movie_emb_path = MODEL_DIR + \"/embeddings_movie_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# director_emb_path = MODEL_DIR + \"/embeddings_director_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# writer_emb_path = MODEL_DIR + \"/embeddings_writer_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# actor_emb_path = MODEL_DIR + \"/embeddings_starred_actor_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# year_emb_path = MODEL_DIR + \"/embeddings_year_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# language_emb_path = MODEL_DIR + \"/embeddings_language_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# tags_emb_path = MODEL_DIR + \"/embeddings_tags_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# genre_emb_path = MODEL_DIR + \"/embeddings_genre_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# votes_emb_path = MODEL_DIR + \"/embeddings_votes_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# rating_emb_path = MODEL_DIR + \"/embeddings_rating_0.v{NUMBER_OF_EPOCHS}.h5\" \\\n",
    "#     .format(NUMBER_OF_EPOCHS=raw_config['num_epochs'])\n",
    "\n",
    "# with open(movies_path, 'r') as f:\n",
    "#     movies = json.load(f)\n",
    "\n",
    "# with h5py.File(movie_emb_path, 'r') as g:\n",
    "#     movie_embeddings = g['embeddings'][:]\n",
    "\n",
    "# movie2embedding = dict(zip(movies, movie_embeddings))\n",
    "# # print('movie embeddings')\n",
    "# # print(movie2embedding)\n",
    "\n",
    "# with open(directors_path, 'r') as f:\n",
    "#     directors = json.load(f)\n",
    "\n",
    "# with h5py.File(director_emb_path, 'r') as g:\n",
    "#     director_embeddings = g['embeddings'][:]\n",
    "\n",
    "# director2embedding = dict(zip(directors, director_embeddings))\n",
    "# # print('director embeddings')\n",
    "# # print(director2embedding)\n",
    "\n",
    "# with open(writers_path, 'r') as f:\n",
    "#     writers = json.load(f)\n",
    "\n",
    "# with h5py.File(writer_emb_path, 'r') as g:\n",
    "#     writer_embeddings = g['embeddings'][:]\n",
    "\n",
    "# writer2embedding = dict(zip(writers, writer_embeddings))\n",
    "# # print('writer embeddings')\n",
    "# # print(writer2embedding)\n",
    "\n",
    "# with open(actors_path, 'r') as f:\n",
    "#     actors = json.load(f)\n",
    "\n",
    "# with h5py.File(actor_emb_path, 'r') as g:\n",
    "#     actor_embeddings = g['embeddings'][:]\n",
    "\n",
    "# actor2embedding = dict(zip(actors, actor_embeddings))\n",
    "# # print('actor embeddings')\n",
    "# # print(actor2embedding)\n",
    "\n",
    "# with open(years_path, 'r') as f:\n",
    "#     years = json.load(f)\n",
    "\n",
    "# with h5py.File(year_emb_path, 'r') as g:\n",
    "#     year_embeddings = g['embeddings'][:]\n",
    "\n",
    "# year2embedding = dict(zip(years, year_embeddings))\n",
    "# # print('year embeddings')\n",
    "# # print(year2embedding)\n",
    "\n",
    "# with open(languages_path, 'r') as f:\n",
    "#     languages = json.load(f)\n",
    "\n",
    "# with h5py.File(language_emb_path, 'r') as g:\n",
    "#     language_embeddings = g['embeddings'][:]\n",
    "\n",
    "# language2embedding = dict(zip(languages, language_embeddings))\n",
    "# # print('language embeddings')\n",
    "# # print(language2embedding)\n",
    "\n",
    "# with open(tags_path, 'r') as f:\n",
    "#     tags = json.load(f)\n",
    "\n",
    "# with h5py.File(tags_emb_path, 'r') as g:\n",
    "#     tags_embeddings = g['embeddings'][:]\n",
    "\n",
    "# tag2embedding = dict(zip(tags, tags_embeddings))\n",
    "# # print('tag embeddings')\n",
    "# # print(tag2embedding)\n",
    "\n",
    "# with open(genres_path, 'r') as f:\n",
    "#     genres = json.load(f)\n",
    "\n",
    "# with h5py.File(genre_emb_path, 'r') as g:\n",
    "#     genre_embeddings = g['embeddings'][:]\n",
    "\n",
    "# genre2embedding = dict(zip(genres, genre_embeddings))\n",
    "# # print('genre embeddings')\n",
    "# # print(genre2embedding)\n",
    "\n",
    "# with open(votes_path, 'r') as f:\n",
    "#     votes = json.load(f)\n",
    "\n",
    "# with h5py.File(votes_emb_path, 'r') as g:\n",
    "#     votes_embeddings = g['embeddings'][:]\n",
    "\n",
    "# votes2embedding = dict(zip(votes, votes_embeddings))\n",
    "# # print('votes embeddings')\n",
    "# # print(votes2embedding)\n",
    "\n",
    "# with open(rating_path, 'r') as f:\n",
    "#     ratings = json.load(f)\n",
    "\n",
    "# with h5py.File(rating_emb_path, 'r') as g:\n",
    "#     rating_embeddings = g['embeddings'][:]\n",
    "\n",
    "# rating2embedding = dict(zip(ratings, rating_embeddings))\n",
    "# # print('rating embeddings')\n",
    "# # print(rating2embedding)\n",
    "\n",
    "# entity2embedding = {**movie2embedding, **director2embedding, **writer2embedding, **actor2embedding, **year2embedding, **language2embedding, **tag2embedding, **genre2embedding, **votes2embedding, **rating2embedding}\n",
    "# print('entity embeddings')\n",
    "# print(entity2embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
